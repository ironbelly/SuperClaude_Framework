# Autonomous Specification Evaluation Prompt

## Objective

Conduct a comprehensive, verifiable evaluation of two specification documents generated by different AI agents from identical prompts. The evaluation must use all available analysis methods including parallel agents, chain of thought reasoning, adversarial debate, and expert panel review.

---

## Input Documents

- **Spec A (SuperClaudeCrossLLM)**: `.dev/releases/v1.2/decoupled-collection-spec-SuperClaudeCrossLLM.md`
- **Spec B (Kiro)**: `.dev/releases/v1.2/decoupled-collection-spec_Kiro.md`
- **Original Prompt**: `.dev/releases/v1.2/spec-generation-prompt.md`

---

## Execution Instructions

**CRITICAL**: Execute all phases in order. Do NOT ask for user input. Make autonomous decisions where judgment is required. Document all reasoning transparently.

---

## Phase 1: Initial Parallel Analysis (Use Task Agents in Parallel)

### 1.1 Spawn THREE parallel agents simultaneously:

**Agent 1 - Completeness Auditor**:
```
Analyze BOTH specs against the original prompt requirements.
For each required section in spec-generation-prompt.md, verify:
- Section present: YES/NO/PARTIAL
- Content quality: FULL/MINIMAL/MISSING
- Examples provided: YES/NO

Output: Completeness matrix for both specs with gap identification.
```

**Agent 2 - Technical Accuracy Reviewer**:
```
Analyze BOTH specs for technical accuracy:
- Architecture diagrams: correctness, completeness, clarity
- JSON schemas: validity, completeness, examples
- Bash script logic: feasibility, error handling, edge cases
- Configuration files: completeness, realistic values
- Gherkin scenarios: coverage, edge cases, testability

Output: Technical accuracy scorecard (1-10) with evidence for each rating.
```

**Agent 3 - Requirements Traceability Analyst**:
```
Map BOTH specs against the original prompt's explicit requirements:
- Design Objectives (6 items): trace to implementation details
- Specification Requirements (11 sections): coverage analysis
- Quality Standards (7 items): compliance verification
- Expert Review Criteria (5 experts): gap analysis

Output: Traceability matrix showing requirement coverage for both specs.
```

---

## Phase 2: Expert Panel Review

### 2.1 Execute /sc:spec-panel on BOTH specifications

**For Spec A**:
```
/sc:spec-panel @.dev/releases/v1.2/decoupled-collection-spec-SuperClaudeCrossLLM.md --experts "wiegers,nygard,adzic,fowler,hightower" --mode discussion
```

Capture:
- Each expert's assessment (strengths and gaps)
- Cross-expert synthesis
- Blind spots identified

**For Spec B**:
```
/sc:spec-panel @.dev/releases/v1.2/decoupled-collection-spec_Kiro.md --experts "wiegers,nygard,adzic,fowler,hightower" --mode discussion
```

Capture same information.

### 2.2 Comparative Expert Synthesis

After both panels complete, synthesize:
- Which spec each expert would prefer and why
- Areas where specs differ in approach
- Combined recommendations for an ideal spec

---

## Phase 3: Chain of Thought Deep Analysis

### 3.1 Use Sequential Thinking MCP (--seq flag)

Execute structured reasoning analysis on BOTH specs using `mcp__sequential-thinking__sequentialthinking`:

**Analysis Questions**:
1. How well does the architecture address the stated problems?
2. Are the failure modes comprehensively handled?
3. Is the migration plan realistic and complete?
4. Are there implicit assumptions that should be explicit?
5. What is missing that an implementer would need?
6. Which spec would be easier to implement?
7. Which spec would be more maintainable long-term?
8. Which spec handles edge cases better?

For each question, use 5-7 thinking steps with hypothesis generation and verification.

---

## Phase 4: Adversarial Debate

### 4.1 Initial Position Establishment

Spawn TWO adversarial debate agents:

**Advocate for Spec A**:
```
You are the advocate for Spec A (SuperClaudeCrossLLM).
Your task: Build the strongest possible case for why Spec A is superior.
- Identify every strength and advantage
- Find evidence in the document to support your claims
- Anticipate and preempt criticisms
- Be specific with line references and examples
```

**Advocate for Spec B**:
```
You are the advocate for Spec B (Kiro).
Your task: Build the strongest possible case for why Spec B is superior.
- Identify every strength and advantage
- Find evidence in the document to support your claims
- Anticipate and preempt criticisms
- Be specific with line references and examples
```

### 4.2 Cross-Examination Rounds

**Round 1**: Each advocate challenges the other's claims
- Spec A Advocate identifies weaknesses in Spec B
- Spec B Advocate identifies weaknesses in Spec A
- Both must provide evidence

**Round 2**: Rebuttals
- Each advocate defends their spec against criticisms
- Must acknowledge valid criticisms

**Round 3**: Final Summation
- Each advocate provides final argument
- Must concede any points where opponent's spec is genuinely better

### 4.3 Neutral Judge Resolution

After debate, spawn a neutral judge agent:
```
You are a neutral technical judge.
Review the adversarial debate transcript.
For each contested point:
1. Who made the stronger argument?
2. What is the objective truth?
3. How should this affect scoring?

Produce: Debate verdict document with point-by-point rulings.
```

---

## Phase 5: Objective Scoring Matrix

### 5.1 Scoring Categories and Weights

| Category | Weight | Subcriteria |
|----------|--------|-------------|
| **Completeness** | 20% | All 11 sections present, all subsections covered, no gaps |
| **Technical Accuracy** | 20% | Correct architecture, valid schemas, feasible scripts |
| **Clarity & Readability** | 10% | Clear prose, good organization, effective diagrams |
| **Testability** | 15% | Gherkin scenarios, acceptance criteria, measurable targets |
| **Implementability** | 15% | Sufficient detail for implementation, no ambiguity |
| **Resilience Design** | 10% | Error handling, circuit breakers, graceful degradation |
| **Security** | 5% | Secrets handling, access control, scoping |
| **Operability** | 5% | Monitoring, logging, procedures, deployment options |

### 5.2 Scoring Rubric (1-10 scale)

```yaml
scoring_rubric:
  1-2: "Critical gaps, missing major elements, unusable"
  3-4: "Significant gaps, partial coverage, needs major revision"
  5-6: "Adequate, covers basics, some gaps or ambiguity"
  7-8: "Good, comprehensive coverage, minor gaps"
  9-10: "Excellent, exceeds requirements, exemplary"
```

### 5.3 Score Assignment Process

For each category:
1. Review relevant sections in both specs
2. Apply rubric criteria
3. Assign preliminary score
4. Document evidence supporting score
5. Flag any uncertain scores for debate verification

---

## Phase 6: Score Verification Through Adversarial Debate

### 6.1 Score Challenge Process

For each scoring category:

**Challenger Agent**:
```
Review the assigned scores for [CATEGORY].
Challenge any score that seems:
- Too high (provide evidence of gaps)
- Too low (provide evidence of quality)
- Inconsistent (compare to similar category scores)

You must find at least ONE challenge per spec if possible.
If no valid challenge exists, state "No valid challenge - score justified."
```

**Defender Agent**:
```
Defend the original scores against challenges.
Provide additional evidence if available.
Concede if the challenge is valid.
Propose adjusted score if warranted.
```

### 6.2 Score Finalization

After all challenges resolved:
1. Compile final scores with adjustments
2. Calculate weighted totals
3. Document any score changes with rationale
4. Produce final comparative matrix

---

## Phase 7: Reflection and Synthesis

### 7.1 Execute /sc:reflect

```
/sc:reflect --target "spec-evaluation" --depth comprehensive

Reflect on:
1. Was the evaluation methodology thorough?
2. Were there any biases detected in the analysis?
3. Are the scores defensible and evidence-based?
4. What would change the evaluation outcome?
5. What are the limits of this evaluation?
```

### 7.2 Final Synthesis Report

Produce a comprehensive report containing:

**Section 1: Executive Summary**
- Winner declaration (or tie with explanation)
- Key differentiators
- One-paragraph verdict

**Section 2: Objective Scoring Matrix**
```markdown
| Category | Weight | Spec A | Spec B | Winner |
|----------|--------|--------|--------|--------|
| ... | ... | X.X | X.X | A/B/Tie |
| **WEIGHTED TOTAL** | 100% | XX.X | XX.X | **A/B** |
```

**Section 3: Subjective Comparison**
- Narrative analysis of differences in approach
- Stylistic and organizational differences
- Areas where one spec excels
- Areas where the other spec excels

**Section 4: Strength-Weakness Analysis**

For Spec A:
- Top 5 Strengths (with evidence)
- Top 5 Weaknesses (with evidence)

For Spec B:
- Top 5 Strengths (with evidence)
- Top 5 Weaknesses (with evidence)

**Section 5: Expert Panel Synthesis**
- Summary of each expert's preference
- Key expert insights

**Section 6: Adversarial Debate Summary**
- Key contested points
- Resolution outcomes
- Points conceded by each side

**Section 7: Recommendations**
- If combining specs: what to take from each
- If choosing one: which and why
- Suggested improvements for the chosen spec

**Section 8: Methodology Transparency**
- All tools and methods used
- Any limitations or caveats
- Confidence level in conclusions

---

## Output Deliverables

Save all outputs to `.dev/releases/v1.2/evaluation/`:

1. `completeness-matrix.md` - Phase 1 Agent 1 output
2. `technical-accuracy-scorecard.md` - Phase 1 Agent 2 output
3. `traceability-matrix.md` - Phase 1 Agent 3 output
4. `expert-panel-specA.md` - Phase 2 Spec A panel
5. `expert-panel-specB.md` - Phase 2 Spec B panel
6. `sequential-analysis.md` - Phase 3 chain of thought
7. `adversarial-debate-transcript.md` - Phase 4 full debate
8. `debate-verdict.md` - Phase 4 judge ruling
9. `preliminary-scores.md` - Phase 5 initial scores
10. `score-verification-debate.md` - Phase 6 challenges
11. `final-scores.md` - Phase 6 finalized scores
12. `reflection-report.md` - Phase 7.1 reflection
13. `FINAL-EVALUATION-REPORT.md` - Phase 7.2 comprehensive synthesis

---

## Execution Commands Sequence

```bash
# Phase 1: Parallel Analysis (run all three simultaneously)
# Use Task tool with subagent_type for parallel execution

# Phase 2: Expert Panels
/sc:spec-panel @.dev/releases/v1.2/decoupled-collection-spec-SuperClaudeCrossLLM.md
/sc:spec-panel @.dev/releases/v1.2/decoupled-collection-spec_Kiro.md

# Phase 3: Sequential Thinking
# Use mcp__sequential-thinking__sequentialthinking tool

# Phase 4: Adversarial Debate
# Use Task tool with debate personas

# Phase 5: Scoring
# Manual scoring with rubric

# Phase 6: Score Verification
# Use Task tool for challenger/defender debate

# Phase 7: Synthesis
/sc:reflect --comprehensive
# Generate final report
```

---

## Quality Gates

Before completing each phase, verify:

- [ ] All outputs are evidence-based with document references
- [ ] No claims without supporting quotes or line numbers
- [ ] All scores have explicit justification
- [ ] Adversarial positions were genuinely contested
- [ ] Expert perspectives are authentically represented
- [ ] Final report is internally consistent
- [ ] Methodology is fully transparent

---

## Autonomous Decision Rules

When judgment is required, apply these rules:

1. **Ties**: If scores are within 0.5 points, declare a tie for that category
2. **Missing sections**: Score as 0 for that component
3. **Partial content**: Score proportionally (50% complete = 5/10 baseline)
4. **Ambiguity**: Flag in report, do not assume
5. **Conflicting evidence**: Present both sides, lean toward documented evidence
6. **Expert disagreement**: Weight by relevance to the specific criterion

---

## Begin Execution

Start with Phase 1: Spawn three parallel analysis agents. Do not wait for user confirmation. Proceed through all phases autonomously, documenting all decisions and reasoning.

**Expected total execution time**: 45-90 minutes
**Expected output**: 13 documents totaling 15,000-25,000 words
